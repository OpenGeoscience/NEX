{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Spark](http://spark.apache.org/images/spark-logo.png)\n",
    "\n",
    "*Apache Sparkâ„¢* is a fast and general engine for large-scale data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we talk about Spark, it is important to consider the entire Berkley Data and Analytics Stack (BDAS).\n",
    "\n",
    "![BDAS Stack](http://i.imgur.com/inQ7N66.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is interesting and exciting, but it __is not magic__. You must know your data and cluster (not to mention Spark itself) to use it effectively. Some key attributes:\n",
    "* Spark is written in Scala\n",
    "* Spark is accessible from Scala, Java, and Python (R too, now).\n",
    "* You can call into native code from Spark.\n",
    "* Everything in Spark boils down to operations on Resilient Distributed Datasets (RDDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The canonical example for MapReduce is word counting. No tutorial would be complete without that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'the', 21)\n",
      "(u'Spark', 14)\n",
      "(u'to', 14)\n",
      "(u'for', 11)\n",
      "(u'and', 10)\n",
      "(u'a', 9)\n",
      "(u'##', 8)\n",
      "(u'run', 7)\n",
      "(u'is', 6)\n",
      "(u'on', 6)\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "text_file = sc.textFile(spark_home + \"/README.md\")\n",
    "\n",
    "# Do the thing\n",
    "word_counts = text_file \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get the top 10 locally\n",
    "top10 = sorted(word_counts.collect(), key=lambda x : -x[1])[:10]\n",
    "\n",
    "# Print the top 10\n",
    "for line in top10:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The real strength of Spark, however, is the plethora of tools built on top of it. MLLib is one of those tools. Let's classify some Iris's. People seem to think that's a really important problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_flower_index(name):\n",
    "    labels = {'Iris-setosa': 0.0,\n",
    "              'Iris-versicolor': 1.0,\n",
    "              'Iris-virginica': 2.0}\n",
    "    return labels[name]\n",
    "\n",
    "x = requests.get('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')\n",
    "rdd = sc.parallelize(x.content.split())\n",
    "raw_flowers = rdd.map(lambda line: line.split(\",\")) \\\n",
    "                 .map(lambda line: [line[4], float(line[0]), float(line[1]), float(line[2]), float(line[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.935483870968\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "flowers = raw_flowers.map(lambda line: LabeledPoint(get_flower_index(line[0]),\n",
    "                                                    Vectors.dense([float(x) for x in line[1:]])))\n",
    "\n",
    "training, test = flowers.randomSplit([0.6, 0.4], seed = 0)\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training, 1.0)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = test.map(lambda p : (model.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classification is cool and all, but I prefer some clustering. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.85        3.07368421  5.74210526  2.07105263]\n",
      "[ 5.006  3.418  1.464  0.244]\n",
      "[ 5.9016129   2.7483871   4.39354839  1.43387097]\n",
      "Within Set Sum of Squared Error = 97.3259242343\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "flower_points = raw_flowers.map(lambda line: Vectors.dense([float(x) for x in line[1:]]))\n",
    "\n",
    "clusters = KMeans.train(flower_points, 3, maxIterations=10, runs=10)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = flower_points.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"\\n\".join([str(x) for x in clusters.centers]))\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all great, but what's so good about all this?\n",
    "\n",
    "The heart of this is the [RDD](https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf). RDDS allow the computing primatives that are used above. Let's talk about this psuedo-code and diagram.\n",
    "\n",
    "```\n",
    "b = a.groupBy()\n",
    "d = c.map()\n",
    "f = d.union(e)\n",
    "g = f.join(b)\n",
    "g.collect()\n",
    "```\n",
    "\n",
    "![RDD](http://i.imgur.com/vrvngXQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the extensions on top of Spark can be thought of as sets of helpful functions and/or specialized RDD implementations. These solutions include (but are not limited to):\n",
    "\n",
    "* [GraphX](http://amplab.github.io/graphx/)\n",
    "* [SparkSQL](http://spark.apache.org/sql/)\n",
    "* [MLLib](https://spark.apache.org/mllib/)\n",
    "* [Spark Streaming](http://spark.apache.org/streaming/)\n",
    "* [BlinkDB](http://blinkdb.org/)\n",
    "\n",
    "The bad news is that there are a lot of these that are not yet available in Python. Maybe we should all start writing Scala?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

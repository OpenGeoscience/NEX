{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fa189af51d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def raise_random_ints_by_two(N, partitions=32):\n",
    "    t0 = time.time()\n",
    "    DATA = np.random.random_integers(0, 10, (N, ))\n",
    "    out = sc.parallelize(DATA, partitions).map(lambda x: x**2).collect()\n",
    "    print \"{}: {}\".format(p, time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: 0.0662159919739\n",
      "32: 0.0676469802856\n",
      "64: 0.0822758674622\n",
      "128: 0.137326955795\n",
      "256: 0.185862064362\n",
      "512: 0.305863857269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[raise_random_ints_by_two(1000, p) for p in [16, 32, 64, 128, 256, 512]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: 0.0904250144958\n",
      "32: 0.0744869709015\n",
      "64: 0.0944068431854\n",
      "128: 0.140532016754\n",
      "256: 0.192093133926\n",
      "512: 0.308105945587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[raise_random_ints_by_two(10000, p) for p in [16, 32, 64, 128, 256, 512]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: 0.129706859589\n",
      "32: 0.126039981842\n",
      "64: 0.129119873047\n",
      "128: 0.185903787613\n",
      "256: 0.241034030914\n",
      "512: 0.361915111542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[raise_random_ints_by_two(100000, p) for p in [16, 32, 64, 128, 256, 512]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: 0.640254020691\n",
      "32: 0.5771048069\n",
      "64: 0.592895030975\n",
      "128: 0.53816986084\n",
      "256: 0.631277799606\n",
      "512: 0.728481054306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[raise_random_ints_by_two(1000000, p) for p in [16, 32, 64, 128, 256, 512]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: 6.87504887581\n",
      "32: 6.04728484154\n",
      "64: 5.30131006241\n",
      "128: 5.08445286751\n",
      "256: 5.0289618969\n",
      "512: 5.4452791214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[raise_random_ints_by_two(10000000, p) for p in [16, 32, 64, 128, 256, 512]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-393d053658d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraise_random_ints_by_two\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-962da9848d22>\u001b[0m in \u001b[0;36mraise_random_ints_by_two\u001b[1;34m(N, partitions)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mDATA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_integers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"{}: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/1.5.1/python/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[0mtempFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[0mreadRDDFromFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadRDDFromFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadRDDFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtempFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/1.5.1/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    }
   ],
   "source": [
    "raise_random_ints_by_two(100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Contour Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debug(s):\n",
    "    # noop here to disable debugging\n",
    "    print s\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def convert(data, variable, timestep):\n",
    "    variable = data.variables[variable]\n",
    "    shape = variable[timestep].shape\n",
    "\n",
    "    # For now sub select ( take about 10% of the grid )\n",
    "    lat_select_index = shape[0]\n",
    "    lon_select_index = shape[1]\n",
    "\n",
    "    # Extract out the lat lon names\n",
    "    dimensions = map(lambda d : d, data.dimensions)\n",
    "    for d in dimensions:\n",
    "        if d.startswith('lat'):\n",
    "            lat_name = d\n",
    "        elif d.startswith('lon'):\n",
    "            lon_name = d\n",
    "\n",
    "    contour_data = {\n",
    "        'gridWidth': lon_select_index,\n",
    "        'gridHeight': lat_select_index,\n",
    "        'x0': float(data.variables[lon_name][0]),\n",
    "        'y0': float(data.variables[lat_name][0]),\n",
    "        'dx': float(data.variables[lon_name][1] - data.variables[lon_name][0]),\n",
    "        'dy': float(data.variables[lat_name][1] - data.variables[lat_name][0]),\n",
    "        'values': variable[timestep][:lat_select_index, :lon_select_index].reshape(variable[timestep][:lat_select_index, :lon_select_index].size).tolist()\n",
    "    }\n",
    "\n",
    "    return contour_data\n",
    "\n",
    "def toNetCDFDataset(source, variable, data):\n",
    "\n",
    "    def _copy_variable(target_file, source_file, variable):\n",
    "        src_var = source_file.variables[variable]\n",
    "        target_var = target_file.createVariable(variable, src_var.datatype, src_var.dimensions)\n",
    "        target_var.setncatts({k: src_var.getncattr(k) for k in src_var.ncattrs()})\n",
    "        target_var[:] = src_var[:]\n",
    "\n",
    "    (fd, filepath) = tempfile.mkstemp()\n",
    "    os.close(fd)\n",
    "    output = Dataset(filepath, 'w')\n",
    "\n",
    "    # Extract out the lat lon names,  these are not\n",
    "    # consistent across the netCDF files\n",
    "    dimensions = map(lambda d : d, source.dimensions)\n",
    "    for d in dimensions:\n",
    "        if d.startswith('lat'):\n",
    "            lat_name = d\n",
    "        elif d.startswith('lon'):\n",
    "            lon_name = d\n",
    "\n",
    "    output.createDimension(lat_name, len(source.dimensions[lat_name])\n",
    "                           if not source.dimensions[lat_name].isunlimited()\n",
    "                           else None)\n",
    "    output.createDimension(lon_name, len(source.dimensions[lon_name])\n",
    "                           if not source.dimensions[lon_name].isunlimited()\n",
    "                           else None)\n",
    "    output.createDimension('time', len(source.dimensions['time'])\n",
    "                           if not source.dimensions['time'].isunlimited()\n",
    "                           else None)\n",
    "    _copy_variable(output, source, lat_name)\n",
    "    _copy_variable(output, source, lon_name)\n",
    "\n",
    "    if type(data) == list:\n",
    "        data_type = data[0].dtype\n",
    "    else:\n",
    "        data_type = data.dtype\n",
    "\n",
    "    output.createVariable(variable, data_type, ('time', lat_name, lon_name))\n",
    "\n",
    "    if type(data) == list:\n",
    "        for i in xrange(len(data)):\n",
    "            output.variables[variable][i] = data[i]\n",
    "    else:\n",
    "        output.variables[variable][:] = data\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def netcdf_mean(filepath, parameter, grid_chunk_size, partitions):\n",
    "    data = Dataset(filepath)\n",
    "    pr = data.variables[parameter]\n",
    "\n",
    "    # Get the number of timesteps\n",
    "    num_timesteps = data.variables['time'].size\n",
    "\n",
    "    # For now don't break up timesteps,  just take mean across\n",
    "    # Grid sections. If we set this to some other value it would\n",
    "    # produce a new dataset with (num_timesteps / timesteps)  new\n",
    "    # panels where each panel was the mean of that group of timesteps\n",
    "    # e.g.,  if timesteps was 10  and num_timesteps was 50 we would have\n",
    "    # 5 panels,  with the average of timesteps 0-10, 10-20, 20-30 etc\n",
    "    timesteps = num_timesteps\n",
    "\n",
    "    # Get number of locations per timestep\n",
    "    shape = pr[0].shape\n",
    "    num_grid_points = pr[0].size\n",
    "\n",
    "    # Break timesteps into n size chunks\n",
    "    timestep_chunks = []\n",
    "\n",
    "    # Break timesteps into n size chunks\n",
    "    timestep_chunks = []\n",
    "    for x in xrange(0, num_timesteps, timesteps):\n",
    "        if x + timesteps < num_timesteps:\n",
    "            timestep_chunks.append((x, x + timesteps))\n",
    "        else:\n",
    "            timestep_chunks.append((x, num_timesteps))\n",
    "\n",
    "\n",
    "    # Break locations into chunks\n",
    "    grid_chunks = []\n",
    "    for lat in xrange(0, shape[0], grid_chunk_size):\n",
    "        for lon in xrange(0, shape[1], grid_chunk_size):\n",
    "            grid_chunks.append((lat, lon))\n",
    "\n",
    "    debug('Grid chunks: %d' % len(grid_chunks))\n",
    "\n",
    "    # Function to process a set of locations for this partition\n",
    "    def calculate_means(grid_chunk):\n",
    "        from netCDF4 import Dataset\n",
    "        import numpy as np\n",
    "        data = Dataset(filepath)\n",
    "        pr = data.variables[parameter]\n",
    "\n",
    "        (lat, lon) = grid_chunk\n",
    "\n",
    "        values = []\n",
    "        for timestep_range in timestep_chunks:\n",
    "            (start_timesteps, end_timesteps) = timestep_range\n",
    "\n",
    "            mean = np.mean(pr[start_timesteps:end_timesteps,\n",
    "                              lat:lat+grid_chunk_size,\n",
    "                              lon:lon+grid_chunk_size], axis=0)\n",
    "            values.append(mean)\n",
    "\n",
    "        return values\n",
    "\n",
    "    # parallelize the grid\n",
    "    grid_chunks = sc.parallelize(grid_chunks, partitions)\n",
    "\n",
    "    # Now calculate means\n",
    "    means = grid_chunks.map(calculate_means)\n",
    "    means = means.collect()\n",
    "\n",
    "    timestep_means = [np.ma.empty(shape) for x in range(len(timestep_chunks))]\n",
    "\n",
    "    i = 0\n",
    "    for lat in xrange(0, shape[0], grid_chunk_size):\n",
    "        for lon in xrange(0, shape[1], grid_chunk_size):\n",
    "            for j in range(len(timestep_chunks)):\n",
    "                chunk = means[i][j]\n",
    "                timestep_means[j][lat:lat+chunk.shape[0], lon:lon+chunk.shape[1]] = chunk\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    return toNetCDFDataset(data, parameter, timestep_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2015-10-20 20:22:48--  http://nasanex.s3.amazonaws.com/NEX-GDDP/BCSD/historical/day/atmos/pr/r1i1p1/v1.0/pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997.nc\n",
      "Resolving nasanex.s3.amazonaws.com (nasanex.s3.amazonaws.com)... 54.231.168.13\n",
      "Connecting to nasanex.s3.amazonaws.com (nasanex.s3.amazonaws.com)|54.231.168.13|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 669108492 (638M) [application/x-netcdf]\n",
      "Saving to: ‘/data/tmp/pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997.nc.2’\n",
      "\n",
      "100%[======================================>] 669,108,492 38.4MB/s   in 14s    \n",
      "\n",
      "2015-10-20 20:23:03 (44.3 MB/s) - ‘/data/tmp/pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997.nc.2’ saved [669108492/669108492]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget --timeout=10 -P /data/tmp/ $(head /home/ubuntu/1997_http_files.txt -n 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 159.5MB/s   00:04    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n",
      "pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997. 100%  638MB 127.6MB/s   00:05    \n"
     ]
    }
   ],
   "source": [
    "! for ip in $(cat /home/ubuntu/ip.list); do scp /data/tmp/* $ip:/data/tmp/; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_mean_contour(grid_chunk_size = 20, partitions = 32):\n",
    "    variable = \"pr\"\n",
    "    file_path = \"/data/tmp/pr_day_BCSD_historical_r1i1p1_ACCESS1-0_1997.nc\"\n",
    "    t0 = time.time()\n",
    "    \n",
    "    data = netcdf_mean(file_path, variable, grid_chunk_size, partitions)\n",
    "    contour = convert(data, variable, 0)\n",
    "    \n",
    "    debug(time.time() - t0)\n",
    "    return time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid chunks: 2592\n",
      "19.5290880203\n"
     ]
    }
   ],
   "source": [
    "run_mean_contour(partitions=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid chunks: 2592\n",
      "11.2182228565\n"
     ]
    }
   ],
   "source": [
    "# Default 32 partitions\n",
    "run_mean_contour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid chunks: 2592\n",
      "7.78277301788\n"
     ]
    }
   ],
   "source": [
    "run_mean_contour(partitions=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid chunks: 2592\n",
      "5.35389208794\n"
     ]
    }
   ],
   "source": [
    "run_mean_contour(partitions=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid chunks: 2592\n",
      "4.77282094955\n"
     ]
    }
   ],
   "source": [
    "run_mean_contour(partitions=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid chunks: 2592\n",
      "4.16521501541\n"
     ]
    }
   ],
   "source": [
    "run_mean_contour(partitions=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid chunks: 2592\n",
      "4.04609704018\n"
     ]
    }
   ],
   "source": [
    "run_mean_contour(partitions=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid chunks: 2592\n",
      "4.16026520729\n"
     ]
    }
   ],
   "source": [
    "run_mean_contour(partitions=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid chunks: 2592\n",
      "4.17134904861\n"
     ]
    }
   ],
   "source": [
    "run_mean_contour(partitions=2592)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.5.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

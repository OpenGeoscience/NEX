{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f28c41c3c10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parquet = sqlContext.read.parquet(\n",
    "    \"hdfs://192.168.33.20/home/ubuntu/day_BCSD_historical_r1i1p1_ACCESS1-0_1997.parquet\")\n",
    "parquet.registerTempTable(\"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+---------+---+---------+---------+\n",
      "|    lat|  lon|     time|    model| pr|   tasmin|   tasmax|\n",
      "+-------+-----+---------+---------+---+---------+---------+\n",
      "|-89.625|0.375|852033600|ACCESS1-0|0.0| 274.4814|277.45853|\n",
      "|-89.625|0.625|852033600|ACCESS1-0|0.0| 274.4868|277.47318|\n",
      "|-89.625|0.875|852033600|ACCESS1-0|0.0|274.50226|277.45074|\n",
      "|-89.625|1.125|852033600|ACCESS1-0|0.0|274.50485|277.44104|\n",
      "|-89.625|1.375|852033600|ACCESS1-0|0.0|274.49878|277.44202|\n",
      "|-89.625|1.625|852033600|ACCESS1-0|0.0|274.49948|277.41565|\n",
      "|-89.625|1.875|852033600|ACCESS1-0|0.0|274.49414|277.38205|\n",
      "|-89.625|2.125|852033600|ACCESS1-0|0.0| 274.4895| 277.3755|\n",
      "|-89.625|2.375|852033600|ACCESS1-0|0.0|274.44693| 277.4029|\n",
      "|-89.625|2.625|852033600|ACCESS1-0|0.0|274.41602|277.42303|\n",
      "|-89.625|2.875|852033600|ACCESS1-0|0.0|274.42282|277.43015|\n",
      "|-89.625|3.125|852033600|ACCESS1-0|0.0|274.42334|277.44162|\n",
      "|-89.625|3.375|852033600|ACCESS1-0|0.0|274.41925| 277.4532|\n",
      "|-89.625|3.625|852033600|ACCESS1-0|0.0| 274.4457| 277.4393|\n",
      "|-89.625|3.875|852033600|ACCESS1-0|0.0|274.48016|277.42258|\n",
      "|-89.625|4.125|852033600|ACCESS1-0|0.0| 274.4657| 277.4395|\n",
      "|-89.625|4.375|852033600|ACCESS1-0|0.0|274.41382| 277.4701|\n",
      "|-89.625|4.625|852033600|ACCESS1-0|0.0| 274.3826| 277.4958|\n",
      "|-89.625|4.875|852033600|ACCESS1-0|0.0|274.39142|277.50082|\n",
      "|-89.625|5.125|852033600|ACCESS1-0|0.0|274.39706|277.50995|\n",
      "+-------+-----+---------+---------+---+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376609324"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_count = parquet.count()\n",
    "parquet_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe\n",
    "Pull out 'month' and 'year' from timestamp and make them available as columns.  Also exclude any datapoints that do not have valid values for pr, tasmin or tasmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT lat, lon, time, model, pr, tasmin, tasmax, MONTH(from_unixtime(time)) as month, YEAR(from_unixtime(time)) as year\n",
    "FROM parquet\n",
    "WHERE pr < 1.0E20 AND tasmin < 1.0E20 AND tasmax < 1.0E20\n",
    "\"\"\"\n",
    "df = sqlContext.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+---------+---+---------+---------+-----+----+\n",
      "|    lat|  lon|     time|    model| pr|   tasmin|   tasmax|month|year|\n",
      "+-------+-----+---------+---------+---+---------+---------+-----+----+\n",
      "|-89.625|0.375|852033600|ACCESS1-0|0.0| 274.4814|277.45853|   12|1996|\n",
      "|-89.625|0.625|852033600|ACCESS1-0|0.0| 274.4868|277.47318|   12|1996|\n",
      "|-89.625|0.875|852033600|ACCESS1-0|0.0|274.50226|277.45074|   12|1996|\n",
      "|-89.625|1.125|852033600|ACCESS1-0|0.0|274.50485|277.44104|   12|1996|\n",
      "|-89.625|1.375|852033600|ACCESS1-0|0.0|274.49878|277.44202|   12|1996|\n",
      "|-89.625|1.625|852033600|ACCESS1-0|0.0|274.49948|277.41565|   12|1996|\n",
      "|-89.625|1.875|852033600|ACCESS1-0|0.0|274.49414|277.38205|   12|1996|\n",
      "|-89.625|2.125|852033600|ACCESS1-0|0.0| 274.4895| 277.3755|   12|1996|\n",
      "|-89.625|2.375|852033600|ACCESS1-0|0.0|274.44693| 277.4029|   12|1996|\n",
      "|-89.625|2.625|852033600|ACCESS1-0|0.0|274.41602|277.42303|   12|1996|\n",
      "|-89.625|2.875|852033600|ACCESS1-0|0.0|274.42282|277.43015|   12|1996|\n",
      "|-89.625|3.125|852033600|ACCESS1-0|0.0|274.42334|277.44162|   12|1996|\n",
      "|-89.625|3.375|852033600|ACCESS1-0|0.0|274.41925| 277.4532|   12|1996|\n",
      "|-89.625|3.625|852033600|ACCESS1-0|0.0| 274.4457| 277.4393|   12|1996|\n",
      "|-89.625|3.875|852033600|ACCESS1-0|0.0|274.48016|277.42258|   12|1996|\n",
      "|-89.625|4.125|852033600|ACCESS1-0|0.0| 274.4657| 277.4395|   12|1996|\n",
      "|-89.625|4.375|852033600|ACCESS1-0|0.0|274.41382| 277.4701|   12|1996|\n",
      "|-89.625|4.625|852033600|ACCESS1-0|0.0| 274.3826| 277.4958|   12|1996|\n",
      "|-89.625|4.875|852033600|ACCESS1-0|0.0|274.39142|277.50082|   12|1996|\n",
      "|-89.625|5.125|852033600|ACCESS1-0|0.0|274.39706|277.50995|   12|1996|\n",
      "+-------+-----+---------+---------+---+---------+---------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o74.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 12.0 failed 4 times, most recent failure: Lost task 23.3 in stage 12.0 (TID 261, data-01.cluster.dev): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:755)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:494)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:283)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:238)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:150)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:36)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:60)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:70)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:60)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegen$$anonfun$6$$anon$1.hasNext(WholeStageCodegen.scala:361)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:78)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:46)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:82)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:231)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:809)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1666)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1625)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1614)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1768)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1781)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1794)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1808)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:877)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:357)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:876)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:280)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2065)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:53)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2352)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2064)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2071)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2099)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2098)\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2365)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2098)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:290)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:755)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:494)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:283)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:238)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:150)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:36)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:60)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:70)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:60)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegen$$anonfun$6$$anon$1.hasNext(WholeStageCodegen.scala:361)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:78)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:46)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:82)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:231)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-49cef03fac5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/78071736799b6c86b5c01b27395f4ab87075342b/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 836\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/78071736799b6c86b5c01b27395f4ab87075342b/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    308\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    309\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o74.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 12.0 failed 4 times, most recent failure: Lost task 23.3 in stage 12.0 (TID 261, data-01.cluster.dev): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:755)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:494)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:283)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:238)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:150)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:36)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:60)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:70)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:60)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegen$$anonfun$6$$anon$1.hasNext(WholeStageCodegen.scala:361)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:78)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:46)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:82)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:231)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:809)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1666)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1625)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1614)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1768)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1781)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1794)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1808)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:877)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:357)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:876)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:280)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2065)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:53)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2352)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2064)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2071)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2099)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2098)\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2365)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2098)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:290)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:755)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:494)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:283)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:238)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:150)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:36)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:60)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:70)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:60)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegen$$anonfun$6$$anon$1.hasNext(WholeStageCodegen.scala:361)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:78)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:46)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:82)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:231)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_count = df.count()\n",
    "df_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percent of cells excluded due to missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.001147077282664409"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df_count - parquet_count) / float(parquet_count)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we've actually removed missing values (encoded as 1.0E20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|max(tasmin)|max(tasmax)|    max(pr)|\n",
      "+-----------+-----------+-----------+\n",
      "|  313.36398|   324.6712|0.008886545|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"pr\", \"tasmin\", \"tasmax\").agg({\"pr\": \"max\", \n",
    "                                     \"tasmin\": \"max\", \n",
    "                                     \"tasmax\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "def _k_to_f(k):\n",
    "    return ((k - 273.15) * 1.8) + 32.0\n",
    "\n",
    "k_to_f = udf(_k_to_f, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o34.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, data-01.cluster.dev): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:809)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1666)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1625)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1614)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1768)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1781)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1794)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\n\tat org.apache.spark.sql.execution.CollectLimit.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2065)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:53)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2352)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2064)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2071)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1838)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1837)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2382)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1837)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2021)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:233)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:290)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-10adb156778a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f_tasmin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_to_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasmin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/spark/master/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \"\"\"\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 836\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/spark/master/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    308\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    309\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o34.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9, data-01.cluster.dev): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:809)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:809)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1666)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1625)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1614)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1768)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1781)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1794)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\n\tat org.apache.spark.sql.execution.CollectLimit.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2065)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:53)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2352)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2064)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2071)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1838)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1837)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2382)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1837)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2021)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:233)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:290)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('f_tasmin', k_to_f(df.tasmin)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /opt/spark/master\n",
      "LESSOPEN: | /usr/bin/lesspipe %s\n",
      "SSH_CLIENT: 10.0.2.2 59886 22\n",
      "LOGNAME: vagrant\n",
      "USER: vagrant\n",
      "PYSPARK_SUBMIT_ARGS: --master spark://head:7077 pyspark-shell\n",
      "PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\n",
      "HOME: /home/vagrant\n",
      "LANG: en_US.UTF-8\n",
      "TERM: xterm-color\n",
      "SHELL: /bin/bash\n",
      "SHLVL: 2\n",
      "XDG_RUNTIME_DIR: /run/user/1000\n",
      "JPY_PARENT_PID: 4454\n",
      "PYTHONPATH: /opt/spark/master/python/\n",
      "TMUX: /tmp/tmux-1000/default,4090,0\n",
      "GIT_PAGER: cat\n",
      "XDG_SESSION_ID: 7\n",
      "_: /usr/local/bin/jupyter\n",
      "LS_COLORS: rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.axa=00;36:*.oga=00;36:*.spx=00;36:*.xspf=00;36:\n",
      "LESSCLOSE: /usr/bin/lesspipe %s %s\n",
      "PYTHONSTARTUP: /opt/spark/master/python/pyspark/shell.py\n",
      "SSH_TTY: /dev/pts/0\n",
      "OLDPWD: /opt/spark/master/sbin\n",
      "CLICOLOR: 1\n",
      "PWD: /public/nex/notebooks\n",
      "MAIL: /var/mail/vagrant\n",
      "SSH_CONNECTION: 10.0.2.2 59886 10.0.2.15 22\n",
      "PAGER: cat\n",
      "TMUX_PANE: %0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print '\\n'.join(k + \": \" + v for k, v in os.environ.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winker scale code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Growing region is assumed to be April 1st through October 31st in the Northern Hemisphere,  and October 1st through April 30th in the Southern Hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grow_season = df.select(\"*\")\\\n",
    "              .where(((df.lat >= 0.0) & (df.month >= 4) & (df.month <= 10)) |\n",
    "                     ((df.lat < 0.0) & (df.month <= 4) & (df.month >= 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110856640"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_count = grow_season.count()\n",
    "gs_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percent decrease of data due to filtering on grow season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-70.56421480793708"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((gs_count - df_count ) / float(df_count)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[lat: float, lon: float, time: bigint, model: string, pr: float, tasmin: float, tasmax: float, month: int, year: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grow_season.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "def _k_to_f(k):\n",
    "    return ((k - 273.15) * 1.8) + 32.0\n",
    "\n",
    "def _avg_tmp_f(_min, _max):\n",
    "    return (_k_to_f(_min) + _k_to_f(_max)) / 2.0\n",
    "\n",
    "avg_tmp_f = udf(_avg_tmp_f, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+------------+------------+---------+---------+-----+----+---------+\n",
      "|  lat|  lon|     time|       model|          pr|   tasmin|   tasmax|month|year| f_tasavg|\n",
      "+-----+-----+---------+------------+------------+---------+---------+-----+----+---------+\n",
      "|0.125|0.375|859896000|IPSL-CM5A-LR| 7.648604E-5|293.99588|294.51733|    4|1997| 69.99189|\n",
      "|0.125|0.625|859896000|IPSL-CM5A-LR| 7.515501E-5| 293.9915|294.52817|    4|1997| 69.99769|\n",
      "|0.125|0.875|859896000|IPSL-CM5A-LR| 7.382399E-5| 293.9925|294.52426|    4|1997| 69.99508|\n",
      "|0.125|1.125|859896000|IPSL-CM5A-LR|  4.55957E-5| 293.9917|294.52222|    4|1997| 69.99252|\n",
      "|0.125|1.375|859896000|IPSL-CM5A-LR|4.4758526E-5| 293.9889|294.52194|    4|1997| 69.98975|\n",
      "|0.125|1.625|859896000|IPSL-CM5A-LR|4.3921355E-5| 293.9902|294.50815|    4|1997|69.978516|\n",
      "|0.125|1.875|859896000|IPSL-CM5A-LR|4.3084186E-5|293.99295|294.48712|    4|1997| 69.96207|\n",
      "|0.125|2.125|859896000|IPSL-CM5A-LR|4.5778543E-5|293.98972| 294.4758|    4|1997| 69.94897|\n",
      "|0.125|2.375|859896000|IPSL-CM5A-LR| 4.487139E-5|293.97952|294.47028|    4|1997| 69.93482|\n",
      "|0.125|2.625|859896000|IPSL-CM5A-LR| 4.396424E-5|293.98038| 294.4716|    4|1997| 69.93677|\n",
      "|0.125|2.875|859896000|IPSL-CM5A-LR|4.3057084E-5|293.99423|294.47867|    4|1997| 69.95561|\n",
      "|0.125|3.125|859896000|IPSL-CM5A-LR| 4.573757E-5| 294.0066|294.48755|    4|1997|69.974724|\n",
      "|0.125|3.375|859896000|IPSL-CM5A-LR|4.4753207E-5|294.01797|  294.497|    4|1997|69.993484|\n",
      "|0.125|3.625|859896000|IPSL-CM5A-LR| 4.376884E-5| 294.0429| 294.5107|    4|1997| 70.02826|\n",
      "|0.125|3.875|859896000|IPSL-CM5A-LR| 4.329148E-5|294.06796| 294.5163|    4|1997| 70.05583|\n",
      "|0.125|4.125|859896000|IPSL-CM5A-LR| 4.535448E-5|294.07797|294.50967|    4|1997| 70.05888|\n",
      "|0.125|4.375|859896000|IPSL-CM5A-LR|4.5385514E-5|294.08084| 294.5132|    4|1997| 70.06465|\n",
      "|0.125|4.625|859896000|IPSL-CM5A-LR|4.5416542E-5| 294.2755| 294.7154|    4|1997|70.421814|\n",
      "|0.125|4.875|859896000|IPSL-CM5A-LR| 4.544757E-5|294.66672| 295.1113|    4|1997| 71.13021|\n",
      "|0.125|5.125|859896000|IPSL-CM5A-LR| 7.576151E-5|295.05615| 295.5093|    4|1997| 71.83891|\n",
      "+-----+-----+---------+------------+------------+---------+---------+-----+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grow_season = grow_season.withColumn('f_tasavg', avg_tmp_f(df.tasmin, df.tasmax))\n",
    "grow_season.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60415559"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grow_season.where(grow_season.f_tasavg > 50.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Winkler_scale\n",
    "def _winkler_scale(d):\n",
    "    if d <= 2500:\n",
    "        return 1\n",
    "    elif d >= 2501 and d <= 3000:\n",
    "        return 2\n",
    "    elif d >= 3001 and d <= 3500:\n",
    "        return 3\n",
    "    elif d >= 3501 and d <= 4000:\n",
    "        return 4\n",
    "    elif d > 4000:\n",
    "        return 5\n",
    "\n",
    "def _degree_days(temp):\n",
    "    dd = int(temp - 50.0)\n",
    "    return 0 if dd <= 0 else dd\n",
    "\n",
    "degree_days = udf(_degree_days, IntegerType())\n",
    "winkler_scale = udf(_winkler_scale, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the degree days\n",
    "Group By year, latitude, longitude and sum the calculated dgree days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = grow_season.withColumn(\"degree_days\", degree_days(grow_season.f_tasavg))\\\n",
    "        .groupBy(df.year, df.lat, df.lon).agg({\"degree_days\": \"sum\"})\\\n",
    "        .withColumnRenamed(\"sum(degree_days)\", \"degree_days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude locations with less than 1 degree day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEGREE_DAY_THRESHOLD = 1\n",
    "dd = dd.where(dd.degree_days >= DEGREE_DAY_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+-----------+\n",
      "|year|  lat|    lon|degree_days|\n",
      "+----+-----+-------+-----------+\n",
      "|1997|0.125|  2.125|       3200|\n",
      "|1997|0.125|  8.125|       5863|\n",
      "|1997|0.125| 14.375|       5902|\n",
      "|1997|0.125| 22.125|       6335|\n",
      "|1997|0.125| 28.375|       5681|\n",
      "|1997|0.125| 34.125|       5461|\n",
      "|1997|0.125| 40.375|       7361|\n",
      "|1997|0.125| 46.625|       3696|\n",
      "|1997|0.125| 52.875|       3740|\n",
      "|1997|0.125| 59.125|       3986|\n",
      "|1997|0.125| 69.875|       4132|\n",
      "|1997|0.125| 76.125|       4175|\n",
      "|1997|0.125| 82.375|       4194|\n",
      "|1997|0.125| 88.625|       4203|\n",
      "|1997|0.125| 94.875|       4289|\n",
      "|1997|0.125|101.125|       6938|\n",
      "|1997|0.125|107.375|       6696|\n",
      "|1997|0.125|113.625|       5528|\n",
      "|1997|0.125|119.875|       6099|\n",
      "|1997|0.125|126.125|       6618|\n",
      "+----+-----+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363817"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdd = dd.withColumn(\"winkler\", winkler_scale(dd.degree_days)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363817"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>degree_days</th>\n",
       "      <th>winkler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>96.375</td>\n",
       "      <td>6653</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>102.625</td>\n",
       "      <td>6602</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>108.875</td>\n",
       "      <td>6806</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>115.125</td>\n",
       "      <td>4584</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>121.375</td>\n",
       "      <td>4942</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>127.625</td>\n",
       "      <td>6628</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>130.375</td>\n",
       "      <td>5007</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>136.625</td>\n",
       "      <td>4475</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>142.875</td>\n",
       "      <td>4467</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>149.125</td>\n",
       "      <td>4389</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>155.375</td>\n",
       "      <td>4404</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>161.625</td>\n",
       "      <td>4499</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>167.875</td>\n",
       "      <td>4377</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>174.125</td>\n",
       "      <td>4444</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>180.375</td>\n",
       "      <td>4329</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>186.625</td>\n",
       "      <td>4251</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>192.875</td>\n",
       "      <td>4157</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>199.125</td>\n",
       "      <td>4339</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>205.375</td>\n",
       "      <td>4040</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>211.625</td>\n",
       "      <td>3882</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>217.875</td>\n",
       "      <td>3800</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>224.125</td>\n",
       "      <td>3691</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>230.375</td>\n",
       "      <td>3594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>236.625</td>\n",
       "      <td>3557</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>242.875</td>\n",
       "      <td>3561</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>249.125</td>\n",
       "      <td>3503</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>255.375</td>\n",
       "      <td>3414</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>257.875</td>\n",
       "      <td>3425</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>264.125</td>\n",
       "      <td>3534</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1997</td>\n",
       "      <td>3.125</td>\n",
       "      <td>270.375</td>\n",
       "      <td>3888</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363787</th>\n",
       "      <td>1997</td>\n",
       "      <td>74.875</td>\n",
       "      <td>112.625</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363788</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.125</td>\n",
       "      <td>90.875</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363789</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.125</td>\n",
       "      <td>97.125</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363790</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.125</td>\n",
       "      <td>103.375</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363791</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.125</td>\n",
       "      <td>109.625</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363792</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.125</td>\n",
       "      <td>138.125</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363793</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.375</td>\n",
       "      <td>94.125</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363794</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.375</td>\n",
       "      <td>100.375</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363795</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.375</td>\n",
       "      <td>106.625</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363796</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.375</td>\n",
       "      <td>112.875</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363797</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.375</td>\n",
       "      <td>138.375</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363798</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.625</td>\n",
       "      <td>97.375</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363799</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.625</td>\n",
       "      <td>103.625</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363800</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.625</td>\n",
       "      <td>109.875</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363801</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.625</td>\n",
       "      <td>138.625</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363802</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.875</td>\n",
       "      <td>94.375</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363803</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.875</td>\n",
       "      <td>100.625</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363804</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.875</td>\n",
       "      <td>106.875</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363805</th>\n",
       "      <td>1997</td>\n",
       "      <td>75.875</td>\n",
       "      <td>113.125</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363806</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.125</td>\n",
       "      <td>97.625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363807</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.125</td>\n",
       "      <td>103.875</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363808</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.125</td>\n",
       "      <td>110.125</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363809</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.375</td>\n",
       "      <td>62.125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363810</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.375</td>\n",
       "      <td>100.875</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363811</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.375</td>\n",
       "      <td>107.125</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363812</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.375</td>\n",
       "      <td>113.375</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363813</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.625</td>\n",
       "      <td>104.125</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363814</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.625</td>\n",
       "      <td>110.375</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363815</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.875</td>\n",
       "      <td>107.375</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363816</th>\n",
       "      <td>1997</td>\n",
       "      <td>77.125</td>\n",
       "      <td>104.375</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363817 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year     lat      lon  degree_days  winkler\n",
       "0       1997   3.125   96.375         6653        5\n",
       "1       1997   3.125  102.625         6602        5\n",
       "2       1997   3.125  108.875         6806        5\n",
       "3       1997   3.125  115.125         4584        5\n",
       "4       1997   3.125  121.375         4942        5\n",
       "5       1997   3.125  127.625         6628        5\n",
       "6       1997   3.125  130.375         5007        5\n",
       "7       1997   3.125  136.625         4475        5\n",
       "8       1997   3.125  142.875         4467        5\n",
       "9       1997   3.125  149.125         4389        5\n",
       "10      1997   3.125  155.375         4404        5\n",
       "11      1997   3.125  161.625         4499        5\n",
       "12      1997   3.125  167.875         4377        5\n",
       "13      1997   3.125  174.125         4444        5\n",
       "14      1997   3.125  180.375         4329        5\n",
       "15      1997   3.125  186.625         4251        5\n",
       "16      1997   3.125  192.875         4157        5\n",
       "17      1997   3.125  199.125         4339        5\n",
       "18      1997   3.125  205.375         4040        5\n",
       "19      1997   3.125  211.625         3882        4\n",
       "20      1997   3.125  217.875         3800        4\n",
       "21      1997   3.125  224.125         3691        4\n",
       "22      1997   3.125  230.375         3594        4\n",
       "23      1997   3.125  236.625         3557        4\n",
       "24      1997   3.125  242.875         3561        4\n",
       "25      1997   3.125  249.125         3503        4\n",
       "26      1997   3.125  255.375         3414        3\n",
       "27      1997   3.125  257.875         3425        3\n",
       "28      1997   3.125  264.125         3534        4\n",
       "29      1997   3.125  270.375         3888        4\n",
       "...      ...     ...      ...          ...      ...\n",
       "363787  1997  74.875  112.625           31        1\n",
       "363788  1997  75.125   90.875           17        1\n",
       "363789  1997  75.125   97.125          131        1\n",
       "363790  1997  75.125  103.375          108        1\n",
       "363791  1997  75.125  109.625           42        1\n",
       "363792  1997  75.125  138.125            7        1\n",
       "363793  1997  75.375   94.125           23        1\n",
       "363794  1997  75.375  100.375          121        1\n",
       "363795  1997  75.375  106.625           58        1\n",
       "363796  1997  75.375  112.875           63        1\n",
       "363797  1997  75.375  138.375           12        1\n",
       "363798  1997  75.625   97.375           42        1\n",
       "363799  1997  75.625  103.625           69        1\n",
       "363800  1997  75.625  109.875            9        1\n",
       "363801  1997  75.625  138.625           14        1\n",
       "363802  1997  75.875   94.375            4        1\n",
       "363803  1997  75.875  100.625           75        1\n",
       "363804  1997  75.875  106.875           17        1\n",
       "363805  1997  75.875  113.125           49        1\n",
       "363806  1997  76.125   97.625            1        1\n",
       "363807  1997  76.125  103.875           46        1\n",
       "363808  1997  76.125  110.125            8        1\n",
       "363809  1997  76.375   62.125            1        1\n",
       "363810  1997  76.375  100.875           12        1\n",
       "363811  1997  76.375  107.125           19        1\n",
       "363812  1997  76.375  113.375           37        1\n",
       "363813  1997  76.625  104.125           23        1\n",
       "363814  1997  76.625  110.375           22        1\n",
       "363815  1997  76.875  107.375            2        1\n",
       "363816  1997  77.125  104.375           27        1\n",
       "\n",
       "[363817 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "winkler\n",
       "1    157663\n",
       "2     19846\n",
       "3     22979\n",
       "4     41634\n",
       "5    121695\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdd.groupby(\"winkler\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdd.to_csv(\"/home/ubuntu/winkler_scale_IPSL-CM5A-LR_1997.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 780717)",
   "language": "python",
   "name": "pyspark_780717"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7fac2c447590>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parquet = sqlContext.read.parquet(\n",
    "    \"hdfs://192.168.33.20/home/ubuntu/day_BCSD_historical_r1i1p1_ACCESS1-0_1997.parquet\")\n",
    "parquet.registerTempTable(\"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376609324"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_count = parquet.count()\n",
    "parquet_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe\n",
    "Pull out 'month' and 'year' from timestamp and make them available as columns.  Also exclude any datapoints that do not have valid values for pr, tasmin or tasmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT lat, lon, time, model, pr, tasmin, tasmax, MONTH(from_unixtime(time)) as month, YEAR(from_unixtime(time)) as year\n",
    "FROM parquet\n",
    "WHERE pr < 1.0E20 AND tasmin < 1.0E20 AND tasmax < 1.0E20\n",
    "\"\"\"\n",
    "df = sqlContext.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_count = df.count()\n",
    "df_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percent of cells excluded due to missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((df_count - parquet_count) / float(parquet_count)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we've actually removed missing values (encoded as 1.0E20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select(\"pr\", \"tasmin\", \"tasmax\").agg({\"pr\": \"max\", \n",
    "                                     \"tasmin\": \"max\", \n",
    "                                     \"tasmax\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "def _k_to_f(k):\n",
    "    return ((k - 273.15) * 1.8) + 32.0\n",
    "\n",
    "k_to_f = udf(_k_to_f, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.withColumn('f_tasmin', k_to_f(df.tasmin)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print '\\n'.join(k + \": \" + v for k, v in os.environ.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winker scale code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Growing region is assumed to be April 1st through October 31st in the Northern Hemisphere,  and October 1st through April 30th in the Southern Hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grow_season = df.select(\"*\")\\\n",
    "              .where(((df.lat >= 0.0) & (df.month >= 4) & (df.month <= 10)) |\n",
    "                     ((df.lat < 0.0) & (df.month <= 4) & (df.month >= 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_count = grow_season.count()\n",
    "gs_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percent decrease of data due to filtering on grow season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((gs_count - df_count ) / float(df_count)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grow_season.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "def _k_to_f(k):\n",
    "    return ((k - 273.15) * 1.8) + 32.0\n",
    "\n",
    "def _avg_tmp_f(_min, _max):\n",
    "    return (_k_to_f(_min) + _k_to_f(_max)) / 2.0\n",
    "\n",
    "avg_tmp_f = udf(_avg_tmp_f, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grow_season = grow_season.withColumn('f_tasavg', avg_tmp_f(df.tasmin, df.tasmax))\n",
    "grow_season.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grow_season.where(grow_season.f_tasavg > 50.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Winkler_scale\n",
    "def _winkler_scale(d):\n",
    "    if d <= 2500:\n",
    "        return 1\n",
    "    elif d >= 2501 and d <= 3000:\n",
    "        return 2\n",
    "    elif d >= 3001 and d <= 3500:\n",
    "        return 3\n",
    "    elif d >= 3501 and d <= 4000:\n",
    "        return 4\n",
    "    elif d > 4000:\n",
    "        return 5\n",
    "\n",
    "def _degree_days(temp):\n",
    "    dd = int(temp - 50.0)\n",
    "    return 0 if dd <= 0 else dd\n",
    "\n",
    "degree_days = udf(_degree_days, IntegerType())\n",
    "winkler_scale = udf(_winkler_scale, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the degree days\n",
    "Group By year, latitude, longitude and sum the calculated dgree days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = grow_season.withColumn(\"degree_days\", degree_days(grow_season.f_tasavg))\\\n",
    "        .groupBy(df.year, df.lat, df.lon).agg({\"degree_days\": \"sum\"})\\\n",
    "        .withColumnRenamed(\"sum(degree_days)\", \"degree_days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude locations with less than 1 degree day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEGREE_DAY_THRESHOLD = 1\n",
    "dd = dd.where(dd.degree_days >= DEGREE_DAY_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdd = dd.withColumn(\"winkler\", winkler_scale(dd.degree_days)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdd.groupby(\"winkler\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdd.to_csv(\"/home/ubuntu/winkler_scale_IPSL-CM5A-LR_1997.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark branch-2.0)",
   "language": "python",
   "name": "pyspark_branch-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "metadata": {
  "name": "AashishOnSpark"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# 10 folder validation for classification using Spark"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "__author__ = 'aashish chaudhary'\n\nfrom cvxopt import matrix, solvers, spmatrix, spdiag, sparse\nimport numpy as np\nimport sys\nimport os\n\n\nspark_home = '/home/aashish/tools/spark/spark'\nos.environ['SPARK_HOME'] = spark_home\n\nsys.path.append(os.path.join(spark_home, 'python'))\nsys.path.append(os.path.join(spark_home, 'bin'))\nsys.path.append(os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n\n# Spark\nfrom pyspark import SparkContext, SparkFiles, SparkConf",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##Prepare possible C values"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "C = []\nval = 0.0\nfor i in xrange(0, 100, 1):\n    C.append(val)\n    val = val + 0.0001",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##Build classifier"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def buildClassifier(data, class_label_1, class_label_2, c):\n    # Now construct matrices\n    n = 0\n    C = 1\n    G = []\n    h = []\n    for items in data:\n        row = []\n        if items[2] == class_label_1:\n            row.extend([-1 * item for item in items[:2]])\n            row.append(-1)\n            G.append(row)\n            h.append(-1.0)\n            n += 1\n        elif items[2] == class_label_2:\n            row.extend(items[:2])\n            row.append(1)\n            G.append(row)\n            h.append(-1.0)\n            n += 1\n        else:\n            continue\n\n    Q = spdiag([1, 1, 0] + [0 for i in range(n)]) * 2\n    p = matrix([0.0, 0.0, 0.0] + [1 for i in range(n)]) * c\n    G = sparse([[matrix(G).trans()], [spdiag([-1 for i in range(n)])]])\n    G1 = sparse([[spmatrix([0], [0], [0], (n, 3))], [spdiag([-1 for i in range(n)])]])\n    G = sparse([G, G1])\n\n    tmp = np.zeros(n)\n    h = matrix(np.hstack((h, tmp)))\n\n    A = matrix([1.0, 1.0, 1.0], (1,3))\n    b = matrix([1.0])\n    A = None\n    b = None\n\n    # print 'Q:\\n', Q\n    # print 'p:\\n', p\n    # print 'G:\\n', G\n    # print 'h:\\n', h\n\n    return [Q, p, G, h]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##Run QP solver"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def solve(class_label_1, class_label_2, classifier, data, result):\n    Q = classifier[0]\n    p = classifier[1]\n    G = classifier[2]\n    h = classifier[3]\n    sol = solvers.qp(Q, p, G, h)\n    w = sol['x'][0:2]\n    b = sol['x'][2]\n\n    # print \"w is\", str(w)\n    # print \"b is\", b\n\n    # Classify now\n    for item in data:\n        if str(item) not in result:\n            result[str(item)] = {class_label_1:0, class_label_2: 0, \"data\":item, \"comp_label\":-1}\n\n        if class_label_1 not in result[str(item)]:\n            result[str(item)][class_label_1] = 0\n\n        if class_label_2 not in result[str(item)]:\n            result[str(item)][class_label_2] = 0\n\n        if np.sign(np.dot(item[0:2], w) + b)[0] < 0:\n            result[str(item)][class_label_2] = result[str(item)][class_label_2] + 1\n        else:\n            result[str(item)][class_label_1] = result[str(item)][class_label_1] + 1",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def chunks(l, n):\n    \"\"\" Yield successive n-sized chunks from l.\n    \"\"\"\n    for i in xrange(0, len(l), n):\n        yield l[i:i+n]\n",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##Compute accuracy"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def computeAccuracy(c):\n    # Read training data\n    data = []\n    for line in open('/home/aashish/tools/spark/svm/tri_training_data.txt').readlines():\n        items = []\n        for item in line.split(','):\n            items.append(float(item))\n        data.append(items)\n\n    chunked_data = list(chunks(data, 100))\n\n    accuracy = 0\n    train_data = []\n\n    for i in range(0, 10):\n        test_data = chunked_data[i]\n\n        for j in range(0, 10):\n            if i != j:\n                train_data.extend(chunked_data[j])\n\n        c_1_vs_2 = buildClassifier(train_data, 1, 2, c)\n        c_2_vs_3 = buildClassifier(train_data, 2, 3, c)\n        c_1_vs_3 = buildClassifier(train_data, 1, 3, c)\n\n        # Now solve it and assign vote to each item\n        result = {}\n        solve(str(1), str(2), c_1_vs_2, test_data, result)\n        solve(str(2), str(3), c_2_vs_3, test_data, result)\n        solve(str(1), str(3), c_1_vs_3, test_data, result)\n\n        for key in result:\n            item = result[key]\n            if (item['1'] > item['2'] and item['1'] > item['3']):\n                result[key][\"comp_label\"] = 1\n            elif (item['2'] > item['1'] and item['2'] > item['3']):\n                result[key][\"comp_label\"] = 2\n            elif (item['3'] > item['1'] and item['3'] > item['2']):\n                result[key][\"comp_label\"] = 3\n\n        # Compute accuracy now\n        right = 0\n        wrong = 0\n        for key in result:\n            item = result[key]\n            if item[\"comp_label\"] != item[\"data\"][2]:\n                wrong += 1.0\n            else:\n                right += 1.0\n        print \"right\", right\n        print \"wrong\", wrong\n        accuracy = accuracy + (float(right) / (right + wrong))\n\treturn accuracy\n",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Create Spark context"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "sc = SparkContext(\"local[4]\", \"Simple App\")",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Run Spark"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "result = sc.parallelize(C).map(computeAccuracy).collect()",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print result\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[0.0, 0.0, 0.0, 0.0, 0.03, 0.36, 0.59, 0.74, 0.79, 0.82, 0.82, 0.84, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.87, 0.88, 0.88, 0.88, 0.88, 0.87, 0.87, 0.88, 0.88, 0.88, 0.89, 0.88, 0.88, 0.88, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.88, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89, 0.89]\n"
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Related Work"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##SciSpark\n\n![SciSpark Architecture](http://i.imgur.com/bbBqnzF.png)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##SparkRS\n Spark-RS, an open source software project that enables GPU-accelerated remote sensing workflows in an Apache Spark distributed computing cluster. Current state-of-the-art parallel systems like Hadoop and Spark offer horizontally scalable analytics and reduced costs for enterprises, but weren't built to natively consume and process large remote sensing raster datasets. Conversely, GPUs can vastly accelerate image processing operations. Some open source projects have arisen that showcase hybrid Hadoop/GPU computing. However, there are no mature open source projects that utilize GPUs within Spark (an eventual replacement of MapReduce) and none that were built to process large remote sensing imagery. This is the primary role of the proposed innovation, Spark-RS.\n\nSpark-RS contains three primary components. One is a parallel large image loading component that quickly loads large multi-band imagery into a Spark cluster. The second component is a remote sensing library for Spark applications. It provides an API for reading and writing large images and wraps many common image operations from existing open source and NASA-built remote sensing libraries. The third component is a GPU management library for Spark. It simplifies and abstracts utilization of GPUs within a Spark application.\n"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Conclusion\n### Spark is popular because 1) It replaces MapReduce 2) Uses memory differently and efficiently 3) Results are impressive\n### Spark for Scientific work will require extension to current Spark\n### We are collaborating with NASA GFSC, JPL, Ames, and ESGF CWT (Multinational Group) on Scientific Spark related work\n### We still have lot to learn on how to use Spark effectively"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}